{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Módulo para aplicação de algoritmos em base dividida em treino e teste</h3>\n",
    "Importação das bibliotecas necessárias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "EDUARDO BISPO FELIZARDO | 10400894\n",
    "GUSTAVO AUGUSTO ALVES PIVATTO | 10401400\n",
    "JEFFERSON DA CONCEICAO DANTAS | 10401327\n",
    "MARINA MIKI SINZATO | 10401880 \n",
    "\n",
    "Objetivo do Notebook\n",
    "O notebook tem como foco a aplicação de modelos preditivos em uma base de dados de vendas de café, \n",
    "com o objetivo de comparar seus desempenhos usando o erro percentual absoluto médio (MAPE).\n",
    "\n",
    "Bibliotecas Importadas\n",
    "O código importa diversas bibliotecas para manipulação de dados, modelagem e visualização, incluindo:\n",
    "\n",
    "pandas, numpy, matplotlib\n",
    "sklearn.metrics para cálculo do MAPE\n",
    "statsmodels para o modelo Holt-Winters (ExponentialSmoothing)\n",
    "xgboost para XGBRegressor\n",
    "pmdarima para uso do AutoARIMA\n",
    "train_test_split para divisão entre treino e teste\n",
    "\n",
    "25/03/2025,EDUARDO BISPO FELIZARDO,Importação de bibliotecas e definição do ambiente de trabalho.\n",
    "25/03/2025,GUSTAVO AUGUSTO ALVES PIVATTO,Leitura da base de dados tratada.\n",
    "26/03/2025,JEFFERSON DA CONCEICAO DANTAS,Divisão da base em treino e teste.\n",
    "27/03/2025,MARINA MIKI SINZATO,Aplicação do modelo Holt-Winters (Exponential Smoothing).\n",
    "27/03/2025,MARINA MIKI SINZATO,Aplicação do modelo XGBoost Regressor.\n",
    "28/03/2025,GUSTAVO AUGUSTO ALVES PIVATTO,Aplicação do modelo AutoARIMA.\n",
    "30/03/2025,JEFFERSON DA CONCEICAO DANTAS,Cálculo e comparação dos valores de MAPE entre os modelos.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "import numpy as np\n",
    "\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from pmdarima import auto_arima\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Carregamento de arquivos</b>\n",
    "<br>\n",
    "Define o caminho do arquivo que contém os dados trataods, que é gerado após rodar o arquivo 2_tratamento_dados.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "caminho_arquivo_base = Path().resolve()  / \"Base de Dados/Base_Atualizada.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base = pd.read_csv(caminho_arquivo_base, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base['date'] = pd.to_datetime(df_base['date'], format='%d/%m/%Y')\n",
    "df_base = df_base.sort_values(by='date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Criação de funções para treinar modelos de previsão de séries temporais usando Holt-Winters (Exponential Smoothing), XGBoost e Auto-ARIMA.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Média Móvel:</b> Técnica de suavização de dados que calcula a média de um conjunto de valores dentro de uma janela móvel ao longo do tempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast_media_movel(df_cafe, train_size, forecast_size):\n",
    "    mape_scores_ma = []\n",
    "\n",
    "    for i in range(0, len(df_cafe) - train_size - forecast_size + 1):\n",
    "        train_set = df_cafe[\"sales\"][i:i + train_size]\n",
    "        test_set = df_cafe[\"sales\"][i + train_size:i + train_size + forecast_size]\n",
    "\n",
    "        df_cafe[\"media_movel\"] = df_cafe[\"sales\"].rolling(window=len(train_set)).mean()\n",
    "        media_movel_forecast = df_cafe[\"media_movel\"].iloc[-len(test_set):]\n",
    "\n",
    "        mape_ma = mean_absolute_percentage_error(test_set, media_movel_forecast)\n",
    "        mape_scores_ma.append(mape_ma)\n",
    "\n",
    "    return mape_scores_ma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Holt-Winters: </b> Modelo de suavização exponencial que considera tendência e sazonalidade em séries temporais. Ele possui três componentes: nível (média ajustada), tendência (crescimento ou declínio) e sazonalidade (padrões repetitivos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast_holtwinters(df_cafe, train_size, forecast_size):\n",
    "    df_cafe['sales'] = df_cafe['sales'].apply(lambda x: x if x > 0 else 0.01)    \n",
    "    mape_scores_hw = []\n",
    "    \n",
    "    grid_params = {\n",
    "        'trend': ['None','add', 'mul'],\n",
    "        'seasonal': ['add', 'mul'],\n",
    "        'seasonal_periods': [1,3,4]\n",
    "    }\n",
    "    \n",
    "    best_params_hw = None \n",
    "    \n",
    "    for i in range(0, len(df_cafe) - train_size - forecast_size + 1):\n",
    "        train_set = df_cafe['sales'][i:i + train_size]\n",
    "        test_set = df_cafe['sales'][i + train_size:i + train_size + forecast_size]\n",
    "        \n",
    "        best_mape = float('inf')\n",
    "        best_params = None\n",
    "        \n",
    "        for trend in grid_params['trend']:\n",
    "            for seasonal in grid_params['seasonal']:\n",
    "                for seasonal_periods in grid_params['seasonal_periods']:\n",
    "                    try:\n",
    "                        model = ExponentialSmoothing(\n",
    "                            train_set,\n",
    "                            trend=trend,\n",
    "                            seasonal=seasonal,\n",
    "                            seasonal_periods=seasonal_periods,\n",
    "                            use_boxcox=True\n",
    "                        )\n",
    "                        fit_model = model.fit()\n",
    "                        forecast = fit_model.forecast(len(test_set))\n",
    "                        \n",
    "                        mape = mean_absolute_percentage_error(test_set, forecast)\n",
    "                        \n",
    "                        if mape < best_mape:\n",
    "                            best_mape = mape\n",
    "                            best_params = (trend, seasonal, seasonal_periods)  \n",
    "                    except:\n",
    "                        continue\n",
    "        \n",
    "        mape_scores_hw.append(best_mape)\n",
    "        if best_params is not None:\n",
    "            best_params_hw = best_params \n",
    "    \n",
    "    return mape_scores_hw, best_params_hw "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>AutoARIMA: </b>Técnica automatizada para identificar o modelo ARIMA (AutoRegressive Integrated Moving Average) mais adequado para uma série temporal. O ARIMA é um modelo amplamente utilizado em séries temporais, composto por três componentes: autoregressivo (AR), de média móvel (MA) e de diferenciação (I), que juntos ajudam a modelar as dependências temporais. O AutoARIMA automatiza o processo de seleção desses parâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast_arima(df_cafe, train_size, forecast_size):\n",
    "\n",
    "    mape_scores_arima = []\n",
    "\n",
    "    for i in range(0, len(df_cafe) - train_size - forecast_size + 1):\n",
    "        train_set = df_cafe['sales'][i:i + train_size]\n",
    "        test_set = df_cafe['sales'][i + train_size:i + train_size + forecast_size]\n",
    "\n",
    "        # Ajuste do modelo ARIMA\n",
    "        model_auto_arima = auto_arima(train_set,\n",
    "                                    start_p=0,\n",
    "                                    start_q=0,\n",
    "                                    start_d=0,\n",
    "                                    max_p=6,\n",
    "                                    max_d=1,\n",
    "                                    max_q=6,\n",
    "                                    seasonal=False,\n",
    "                                    trace=False,\n",
    "                                    suppress_warnings=True,\n",
    "                                    stepwise=True,\n",
    "                                    max_iter=50)\n",
    "\n",
    "        arima_forecast = model_auto_arima.predict(n_periods=len(test_set))\n",
    "        \n",
    "        arima_forecast = arima_forecast[:len(test_set)].reset_index(drop=True)\n",
    "        test_set = test_set.reset_index(drop=True)\n",
    "\n",
    "        filtro = test_set > 0\n",
    "        test_set = test_set[filtro].reset_index(drop=True)\n",
    "        arima_forecast = arima_forecast[filtro].reset_index(drop=True)\n",
    "\n",
    "        mape_arima = mean_absolute_percentage_error(test_set, arima_forecast)\n",
    "        mape_scores_arima.append(mape_arima)\n",
    "\n",
    "    return mape_scores_arima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>XGBoost com Lags: </b>O uso de lags no XGBoost em séries temporais envolve criar variáveis que representam os valores de uma série em pontos anteriores no tempo, ajudando o modelo a capturar dependências temporais. Essas variáveis de lag são usadas como entradas no modelo, permitindo que o XGBoost preveja o valor futuro com base nas observações passadas, o que é essencial para capturar padrões e tendências temporais em dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lag_features(df_cafe, lags):\n",
    "    df_lagged = df_cafe.copy()\n",
    "    \n",
    "    for lag in range(1, lags + 1):\n",
    "        df_lagged[f'lag_{lag}'] = df_lagged['sales'].shift(lag)\n",
    "    \n",
    "    df_lagged = df_lagged.dropna().reset_index(drop=True)  \n",
    "    \n",
    "    return df_lagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast_xgboost_lags(df_cafe, train_size, forecast_size):\n",
    "    df_lagged = create_lag_features(df_cafe[['date', 'sales']], lags=6)\n",
    "    X = df_lagged.drop(columns=['date', 'sales'])\n",
    "    y = df_lagged['sales']\n",
    "    \n",
    "    mape_scores_lags = []\n",
    "\n",
    "    for i in range(0, len(df_lagged) - train_size - forecast_size + 1):\n",
    "        X_train = X.iloc[i:i + train_size]\n",
    "        y_train = y.iloc[i:i + train_size]\n",
    "        X_test = X.iloc[i + train_size:i + train_size + forecast_size]\n",
    "        y_test = y.iloc[i + train_size:i + train_size + forecast_size]\n",
    "\n",
    "        # Ajuste do modelo XGBoost\n",
    "        model = XGBRegressor(random_state=42, \n",
    "                             n_estimators=1500, \n",
    "                             learning_rate=0.05, \n",
    "                             subsample=0.85,\n",
    "                             max_depth=4, \n",
    "                             reg_alpha=0.05, \n",
    "                             reg_lambda=1, \n",
    "                             objective='reg:squarederror'\n",
    "                             )\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        xgboost_forecast = model.predict(X_test)\n",
    "\n",
    "        mape_lags = mean_absolute_percentage_error(y_test, xgboost_forecast)\n",
    "        mape_scores_lags.append(mape_lags)\n",
    "    \n",
    "    return mape_scores_lags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_dados = 24\n",
    "cafes_com_dados = df_base.groupby('coffee_name').size()\n",
    "\n",
    "cafes_filtrados = cafes_com_dados[cafes_com_dados >= min_dados].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Americano', 'Americano with Milk', 'Cappuccino', 'Cocoa', 'Cortado',\n",
       "       'Espresso', 'Hot Chocolate', 'Latte'],\n",
       "      dtype='object', name='coffee_name')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cafes_filtrados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Criação de Loop para iterar por todos os tipos de café</b><br>\n",
    "Calculando e exibindo o valor do MAPE (Mean Absolute Percentage Error) para cada tipo de café."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "train_size = 6 \n",
    "forecast_size = 1\n",
    "  \n",
    "resultados = []\n",
    "\n",
    "for cafe in cafes_filtrados:\n",
    "    df_cafe = df_base[df_base[\"coffee_name\"] == cafe]\n",
    "\n",
    "    df_cafe = df_cafe.groupby(df_cafe[\"date\"].dt.to_period(\"M\")).size().reset_index(name=\"sales\")\n",
    "        \n",
    "    mape_holtwinters, best_params_hw = forecast_holtwinters(df_cafe, train_size, forecast_size)\n",
    "    mape_media_movel = forecast_media_movel(df_cafe, train_size, forecast_size)\n",
    "    mape_xgboost_lags = forecast_xgboost_lags(df_cafe, train_size, forecast_size)\n",
    "    mape_arima = forecast_arima(df_cafe, train_size, forecast_size)\n",
    "\n",
    "    resultados.append((\n",
    "        cafe, \n",
    "        \"holtwinters\", \n",
    "        np.mean(mape_holtwinters), \n",
    "        best_params_hw[0] if best_params_hw else None,\n",
    "        best_params_hw[1] if best_params_hw else None,\n",
    "        best_params_hw[2] if best_params_hw else None\n",
    "    ))\n",
    "\n",
    "    resultados.append((\n",
    "        cafe, \n",
    "        \"media_movel\", \n",
    "        np.mean(mape_media_movel),\n",
    "        None, \n",
    "        None, \n",
    "        None\n",
    "    ))\n",
    "\n",
    "    resultados.append((\n",
    "        cafe, \n",
    "        \"auto_arima\", \n",
    "        np.mean(mape_arima),\n",
    "        None, \n",
    "        None, \n",
    "        None\n",
    "    ))\n",
    "\n",
    "    resultados.append((\n",
    "        cafe, \n",
    "        \"xgboost\", \n",
    "        np.mean(mape_xgboost_lags),\n",
    "        None, \n",
    "        None, \n",
    "        None\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resultados = pd.DataFrame(resultados, columns=[\"coffee_name\", \"modelo\", \"MAPE\", \"trend\", \"seasonal\", \"seasonal_periods\"])\n",
    "df_resultados.to_csv('Bases Geradas/resultados_mape.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
